# -*- coding: utf-8 -*-
"""AI-Powered Audio Transcription Platform.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VwAM1R4LiZX0AMgE99BLqHCJTsmP2MoQ

# Install Dependencies
"""

# Install required packages
!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
!pip install -q openai-whisper==20240930
!apt-get -y install ffmpeg
!pip install -q transformers==4.45.2 accelerate librosa soundfile gradio

"""# Import Libraries"""

import torch
import whisper
import gradio as gr
import os
import numpy as np

"""# Load Models"""

# Check available device
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("âœ… Device:", DEVICE)

# Load Whisper model
MODEL_SIZE = "medium"  # You can change to "small", "large", or "large-v3"
model = whisper.load_model(MODEL_SIZE, device=DEVICE)
print(f"âœ… Whisper model ({MODEL_SIZE}) loaded successfully.")

"""# Transcribe audio cell"""

def transcribe_audio(audio_path):
    if not audio_path:
        return "No audio provided.", "unknown", "0.00"

    try:

        result = model.transcribe(audio_path)

        text = result.get("text", "").strip()
        lang = result.get("language", "unknown")


        segments = result.get("segments", [])
        if segments and len(segments) > 0:
            confidence = "0.85"
        else:
            confidence = "0.00"

        return text, lang, confidence

    except Exception as e:
        return f"Error: {str(e)}", "error", "0.00"

"""# Audio file & Mic"""

def process(audio_file, mic_audio):
    path = audio_file or mic_audio
    if not path:
        return "No audio provided.", "unknown", "0.00"
    return transcribe_audio(path)

def clear_all():
    return None, None, "", "", ""

"""# Dashboard"""

with gr.Blocks() as demo:
    gr.Markdown("# ğŸ¯ Whisper Audio Transcription")

    with gr.Row():
        with gr.Column():
            audio_input = gr.Audio(
                sources=["upload", "microphone"],
                type="filepath",
                label="Upload or Record Audio"
            )

            transcribe_btn = gr.Button("Transcribe Audio", variant="primary")

    with gr.Row():
        text_output = gr.Textbox(label="Transcribed Text", lines=3)

    with gr.Row():
        lang_output = gr.Textbox(label="Detected Language")
        conf_output = gr.Textbox(label="Confidence")

    # Connect button to function
    transcribe_btn.click(
        fn=process,
        inputs=[audio_input],
        outputs=[text_output, lang_output, conf_output]
    )

   # Footer
    gr.HTML("""
<div align="center">

**ğŸ§‘â€ğŸ’»ğŸ§‘â€ğŸ’»ğŸ§‘â€ğŸ’»ğŸ§‘â€ğŸ’» **
<h3>ğŸ§‘â€ğŸ’» Developed by :Md. Ferdaus HossenğŸ§‘â€ğŸ’»</h3>
<h5>Junior AI/ML Engineer at Zensoft Lab</h5>

<p>
  <a href="https://github.com/Ferdaus71" target="_blank" style="margin-right:10px;">
    <img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/github.svg" width="25" height="25" alt="GitHub">
  </a>
  <a href="https://www.linkedin.com/in/ferdaus70/" target="_blank" style="margin-left:10px;">
    <img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/linkedin.svg" width="25" height="25" alt="LinkedIn">
  </a>
</p>

</div>
""")

# Launch the application
print("ğŸš€ Starting the application...")
demo.launch(share=True)